<?xml version="1.0" encoding="UTF-8" ?><ChoregrapheProject xmlns="http://www.aldebaran-robotics.com/schema/choregraphe/project.xsd" xar_version="3"><Box name="root" id="-1" localization="8" tooltip="Root box of Choregraphe&apos;s behavior. Highest level possible." x="0" y="0"><bitmap>media/images/box/root.png</bitmap><script language="4"><content><![CDATA[]]></content></script><Input name="onLoad" type="1" type_size="1" nature="0" inner="1" tooltip="Signal sent when diagram is loaded." id="1" /><Input name="onStart" type="1" type_size="1" nature="2" inner="0" tooltip="Box behavior starts when a signal is received on this input." id="2" /><Input name="onStop" type="1" type_size="1" nature="3" inner="0" tooltip="Box behavior stops when a signal is received on this input." id="3" /><Output name="onStopped" type="1" type_size="1" nature="1" inner="0" tooltip="Signal sent when box behavior is finished." id="4" /><Timeline enable="0"><BehaviorLayer name="behavior_layer1"><BehaviorKeyframe name="keyframe1" index="1"><Diagram><Box name="Face Detection" id="1" localization="8" tooltip="Detect people&apos;s face and return the number of detected faces.&#x0A;&#x0A;Note: Detect even faces that are not registered in the faces database (that&#x0A;you can teach him with the Learn Face box)." x="167" y="13"><bitmap>media/images/box/interaction/face.png</bitmap><script language="4"><content><![CDATA[class MyClass(GeneratedClass):
    def __init__(self):
        GeneratedClass.__init__(self, False)

    def onLoad(self):
        self.bIsRunning = False

    def onUnload(self):
        self.bIsRunning = False

    def onInput_onStart(self):
        self.bIsRunning = True

    def onInput_onStop(self):
        if( self.bIsRunning ):
            self.onUnload()
            self.onStopped()]]></content></script><Input name="onLoad" type="1" type_size="1" nature="0" inner="1" tooltip="Signal sent when diagram is loaded." id="1" /><Input name="onStart" type="1" type_size="1" nature="2" inner="0" tooltip="Box behavior starts when a signal is received on this input." id="2" /><Input name="onStop" type="1" type_size="1" nature="3" inner="0" tooltip="Box behavior stops when a signal is received on this input." id="3" /><Input name="FaceDetected" type="0" type_size="1" nature="4" stm_value_name="FaceDetected" inner="1" tooltip="Connected to ALMemory. Will be stimulated every time the value subscribed to changes, respecting the refresh period." id="4" /><Output name="onStopped" type="1" type_size="1" nature="1" inner="0" tooltip="Signal sent when box behavior is stopped." id="5" /><Output name="numberOfFaces" type="2" type_size="1" nature="2" inner="0" tooltip="Number of detected faces. This output is stimulated each time the number of&#x0A;detected faces change." id="6" /><Output name="onNoFace" type="1" type_size="1" nature="2" inner="0" tooltip="No face is detected." id="7" /><Timeline enable="0"><BehaviorLayer name="behavior_layer1"><BehaviorKeyframe name="keyframe1" index="1"><Diagram><Box name="Count Det. Faces" id="3" localization="8" tooltip="Process face detection extractor data (FaceDetected) to count the number&#x0A;of detected faces and notify when there is no face detected.&#x0A;&#x0A;An output (either one or the other) is stimulated each time the number of&#x0A;detected faces change." x="174" y="71"><bitmap>media/images/box/interaction/reco_face.png</bitmap><script language="4"><content><![CDATA[class MyClass(GeneratedClass):
    def __init__(self):
        GeneratedClass.__init__(self, False)

    def onLoad(self):
        self.nFacesDetected = -1

    def onUnload(self):
        #puts code for box cleanup here
        pass

    def onInput_onStart(self, p):
        if(len(p) > 0):
            if(self.nFacesDetected != len(p[1]) -1): # an additional array has been placed at the end for time
                self.nFacesDetected = len(p[1]) -1  # filtered info and has to be substracted when counting faces
                if(self.nFacesDetected != 0):
                    self.onFaceDetected( self.nFacesDetected )
                else:
                    self.onNoFace()
        else:
            if(self.nFacesDetected != 0):
                self.nFacesDetected = 0
                self.onNoFace()

    def onInput_onStop(self):
        pass]]></content></script><Input name="onLoad" type="1" type_size="1" nature="0" inner="1" tooltip="Signal sent when diagram is loaded." id="1" /><Input name="onStart" type="0" type_size="1" nature="2" inner="0" tooltip="Box behavior starts when a signal is received on this input. It must be the&#x0A;FaceDetected extractor data." id="2" /><Output name="onFaceDetected" type="2" type_size="1" nature="1" inner="0" tooltip="Number of detected faces." id="3" /><Output name="onNoFace" type="1" type_size="1" nature="1" inner="0" tooltip="No face is detected." id="4" /></Box><Link inputowner="3" indexofinput="2" outputowner="0" indexofoutput="4" /><Link inputowner="0" indexofinput="6" outputowner="3" indexofoutput="3" /><Link inputowner="0" indexofinput="7" outputowner="3" indexofoutput="4" /></Diagram></BehaviorKeyframe></BehaviorLayer></Timeline></Box><Box name="Dialog" id="2" localization="8" tooltip="An example of multilanguage dialog implementation" x="366" y="20"><dialogFile>../dialog/dialog.dlg</dialogFile><bitmap>media/images/box/box-dialog.png</bitmap><script language="4"><content><![CDATA[]]></content></script><Input name="onLoad" type="1" type_size="1" nature="0" inner="1" tooltip="Signal sent when diagram is loaded." id="1" /><Input name="onStart" type="1" type_size="1" nature="2" inner="0" tooltip="Box behavior starts when a signal is received on this input." id="2" /><Input name="onStop" type="1" type_size="1" nature="3" inner="0" tooltip="Box behavior stops when a signal is received on this input." id="3" /><Output name="onStopped" type="1" type_size="1" nature="1" inner="0" tooltip="Signal sent when box behavior is finished." id="4" /><Output name="playAGame" type="1" type_size="1" nature="2" inner="0" tooltip="stimulated using $playAGame=1 in qiChat script when User wants to play a game&#x0A;" id="5" /><Resource name="Speech" type="Lock" timeout="0" /><Resource name="Speech recognition" type="Lock" timeout="0" /></Box><Box name="Get Expression (Multi)" id="4" localization="8" tooltip="This box returns the detected facial expression of every person in front of the robot.&#x0A;The detection fails when there is no person in front of the robot or when the timeout is exceeded.&#x0A;&#x0A;It is possible to set up the Confidence Threshold and the Timeout parameters for this box. &#x0A;Furthermore it is possible to select the required emotions:&#x0A;- neutral&#x0A;- happy&#x0A;- surprised&#x0A;- angry&#x0A;- sad" x="530" y="29"><bitmap>media/images/box/interaction/emotion.png</bitmap><script language="4"><content><![CDATA[class MyClass(GeneratedClass):
    def __init__(self):
        GeneratedClass.__init__(self)

    def onLoad(self):
        try:
            self.faceC = ALProxy("ALFaceCharacteristics")
        except Exception as e:
            raise RuntimeError(str(e) + "Make sure you're not connected to a virtual robot." )
        self.confidence = self.getParameter("Confidence Threshold")
        self.threshNeutralEmotion = self.confidence + 0.15
        self.threshHappyEmotion = self.confidence
        self.threshSurprisedEmotion = self.confidence + 0.05
        self.threshAngryEmotion = self.confidence + 0.2
        self.threshSadEmotion = self.confidence + 0.15
        self.emotions = ["neutral", "happy", "surprised", "angry", "sad"]
        self.counter = 0
        self.bIsRunning = False
        self.delayed = []
        self.errorMes = ""

    def onUnload(self):
        self.counter = 0
        self.bIsRunning = False
        self.emotionValues = []
        self.cancelDelays()

    def onInput_onStart(self):
        try:
            #start timer
            import qi
            import functools
            delay_future = qi.async(self.onTimeout, delay=int(self.getParameter("Timeout (s)") * 1000 * 1000))
            self.delayed.append(delay_future)
            bound_clean = functools.partial(self.cleanDelay, delay_future)
            delay_future.addCallback(bound_clean)
            self.emotionValues = []
            self.bIsRunning = True
            while self.bIsRunning:
                if self.counter < 4:
                    try:
                        #identify user
                        ids = ALMemory.getData("PeoplePerception/PeopleList")
                        if len(ids) == 0:
                            self.errorMes = "No face detected"
                            self.onUnload()
                        else:
                            #analyze age properties
                            index = -1
                            for id in ids:
                                index += 1
                                self.faceC.analyzeFaceCharacteristics(id)
                                time.sleep(0.1)
                                value = ALMemory.getData("PeoplePerception/Person/"+str(id)+"/ExpressionProperties")
                                elem = [x for x in self.emotionValues if x[0] == id]
                                if elem !=[]:
                                    elemIndex = self.emotionValues.index(elem[0])
                                    self.emotionValues[elemIndex][1][0] += value[0]
                                    self.emotionValues[elemIndex][1][1] += value[1]
                                    self.emotionValues[elemIndex][1][2] += value[2]
                                    self.emotionValues[elemIndex][1][3] += value[3]
                                    self.emotionValues[elemIndex][1][4] += value[4]
                                    self.emotionValues[elemIndex][2] += 1
                                else:
                                    self.emotionValues.append([id, value, 1])
                                self.counter += 1
                    except Exception as e:
                        ids = []
                else:
                    #calculate mean value for neutral, happy, surprised, angry or sad
                    results = []
                    recognized = [0,0,0,0,0]
                    for res in self.emotionValues:
                        emotions = res[1]
                        emotions[0] /= res[2]
                        emotions[1] /= res[2]
                        emotions[2] /= res[2]
                        emotions[3] /= res[2]
                        emotions[4] /= res[2]
                        if self.getParameter("neutral") and emotions[0] > self.threshNeutralEmotion:
                            recognized[0] = emotions[0]
                        if self.getParameter("happy") and emotions[1] >self.threshHappyEmotion:
                            recognized[1] = emotions[1]
                        if self.getParameter("surprised") and emotions[2] > self.threshSurprisedEmotion:
                            recognized[2] = emotions[2]
                        if self.getParameter("angry") and emotions[3] > self.threshAngryEmotion:
                            recognized[3] = emotions[3]
                        if self.getParameter("sad") and emotions[4] > self.threshSadEmotion:
                            recognized[4] = emotions[4]

                        try:
                            if recognized != [0,0,0,0,0]:
                                emotion = self.emotions[recognized.index(max(recognized))]
                            else:
                                emotion = None
                        except:
                            emotion = None
                        if emotion != None:
                            results.append([res[0], emotion])
                    try:
                        ALMemory.removeData("PeoplePerception/Person/"+str(ids[0])+"/ExpressionProperties")
                    except:
                        pass
                    if results != []:
                        self.onStopped(results)
                        self.onUnload()
                        return
                    else:
                        self.counter = 0

            raise RuntimeError(self.errorMes)
        except Exception as e:
            raise RuntimeError(str(e))
            self.onUnload()

    def onTimeout(self):
        self.errorMes = "Timeout"
        self.onUnload()

    def cleanDelay(self, fut, fut_ref):
        self.delayed.remove(fut)

    def cancelDelays(self):
        cancel_list = list(self.delayed)
        for d in cancel_list:
            d.cancel()

    def onInput_onStop(self):
        self.onUnload()]]></content></script><Input name="onLoad" type="1" type_size="1" nature="0" inner="1" tooltip="Signal sent when diagram is loaded." id="1" /><Input name="onStart" type="1" type_size="1" nature="2" inner="0" tooltip="Box behavior starts when a signal is received on this input." id="2" /><Input name="onStop" type="1" type_size="1" nature="3" inner="0" tooltip="Box behavior stops when a signal is received on this input." id="3" /><Output name="onStopped" type="0" type_size="1" nature="1" inner="0" tooltip='Returns an array containing [userId, emotion] for every user. The userId is the ID of the person as defined by the ALPeoplePerception API and emotion can be: &#x0A;- &quot;neutral&quot;&#x0A;- &quot;happy&quot;&#x0A;- &quot;surprised&quot;&#x0A;- &quot;angry&quot;&#x0A;- &quot;sad&quot;&#x0A;' id="4" /><Output name="onError" type="3" type_size="1" nature="1" inner="0" tooltip='Triggered when gender detection failed. &#x0A;Possible error messages:&#x0A;- &quot;No face detected&quot;&#x0A;- &quot;Timeout&quot;' id="5" /><Parameter name="Confidence Threshold" inherits_from_parent="0" content_type="2" value="0.35" default_value="0.6" min="0" max="1" tooltip="Set the confidence threshold for the age detection." id="6" /><Parameter name="Timeout (s)" inherits_from_parent="0" content_type="2" value="20" default_value="5" min="1" max="60" tooltip="" id="7" /><Parameter name="neutral" inherits_from_parent="0" content_type="0" value="1" default_value="1" tooltip="" id="8" /><Parameter name="happy" inherits_from_parent="0" content_type="0" value="1" default_value="1" tooltip="" id="9" /><Parameter name="surprised" inherits_from_parent="0" content_type="0" value="1" default_value="1" tooltip="" id="10" /><Parameter name="angry" inherits_from_parent="0" content_type="0" value="1" default_value="1" tooltip="" id="11" /><Parameter name="sad" inherits_from_parent="0" content_type="0" value="1" default_value="1" tooltip="" id="12" /></Box><Box name="Sound Tracker" id="3" localization="-1" tooltip="This box makes the robot track a sound with different modes.&#x0A;&#x0A;V1.1.0" x="166" y="164"><bitmap>media/images/box/interaction/target_sound.png</bitmap><script language="4"><content><![CDATA[import time

class MyClass(GeneratedClass):
    def __init__(self):
        GeneratedClass.__init__(self)
        self.tracker = ALProxy( "ALTracker" )
        self.memory = ALProxy("ALMemory")
        try:
            self.soundLocation = ALProxy("ALSoundLocalization")
        except Exception as e:
            self.soundLocation = None
            self.logger.error(e)
        self.targetName = "Sound"
        self.distance = 0.0
        self.soundDistance = 0.5
        self.confidence = 0.0
        self.thresholdX = 0.0
        self.effector = "None"
        self.sensitivity = 0.7
        self.subscribeDone = False
        self.isRunning = False

    def onLoad(self):
        self.BIND_PYTHON(self.getName(), "setParameter")
        self.BIND_PYTHON(self.getName(), "onTargetLost")
        self.BIND_PYTHON(self.getName(), "onTargetReached")
        self.BIND_PYTHON(self.getName(), "onTargetChanged")
        if self.soundLocation:
            self.memory.subscribeToEvent("ALTracker/ActiveTargetChanged", self.getName(), "onTargetChanged")

    def onUnload(self):
        if self.subscribeDone:
            self.memory.unsubscribeToEvent("ALTracker/TargetLost", self.getName())
            self.memory.unsubscribeToEvent("ALTracker/TargetReached", self.getName())
            self.subscribeDone = False

        if self.isRunning:
            self.tracker.setEffector("None")
            self.tracker.stopTracker()
            self.tracker.unregisterTarget(self.targetName)
            self.isRunning = False

    def onInput_onStart(self):
        self.memory.subscribeToEvent("ALTracker/TargetLost", self.getName(), "onTargetLost")
        self.memory.subscribeToEvent("ALTracker/TargetReached", self.getName(), "onTargetReached")
        self.subscribeDone = False

        mode = self.getParameter("Mode")
        self.confidence = self.getParameter("Threshold to be sure of the location (%)") / 100.0
        self.distance = self.getParameter("Distance (m)")
        self.thresholdX = self.getParameter("Threshold X (m)")
        self.effector = self.getParameter("Effector")
        self.sensitivity = self.getParameter("Sensitivity")

        if self.soundLocation:
            self.soundLocation.setParameter("Sensitivity", self.sensitivity)

        self.tracker.setEffector(self.effector)

        self.tracker.registerTarget(self.targetName, [self.soundDistance, self.confidence])
        self.tracker.setRelativePosition([-self.distance, 0.0, 0.0,
                                            self.thresholdX, 0.1, 0.3])
        self.tracker.setMode(mode)

        self.tracker.track(self.targetName) #Start tracker
        self.isRunning = True

    def onInput_onStop(self):
        self.onStopped()
        self.onUnload()

    def setParameter(self, parameterName, newValue):
        GeneratedClass.setParameter(self, parameterName, newValue)
        if (parameterName == 'Mode'):
            self.tracker.setMode(newValue)
            return

        if (parameterName == "Threshold to be sure of the location (%)"):
            self.confidence = self.getParameter("Threshold to be sure of the location (%)") / 100.0
            self.tracker.registerTarget(self.targetName, [self.soundDistance, self.confidence])
            return

        if (parameterName == "Distance (m)"):
            self.distance = newValue
            self.tracker.setRelativePosition([-self.distance, 0.0, 0.0,
                                                self.thresholdX, 0.1, 0.3])
            return

        if (parameterName == "Threshold X (m)"):
            self.thresholdX = newValue
            self.tracker.setRelativePosition([-self.distance, 0.0, 0.0,
                                                self.thresholdX, 0.1, 0.3])
            return

        if(parameterName == "Effector"):
            self.tracker.setEffector(newValue)
            self.effector = newValue
            return

        if(parameterName == "Sensitivity"):
            self.sensitivity = newValue
            if self.soundLocation:
                self.soundLocation.setParameter("Sensitivity", self.sensitivity)
            return


    def onTargetLost(self, key, value, message):
        self.targetLost()

    def onTargetReached(self, key, value, message):
        self.targetReached()

    def onTargetChanged(self, key, value, message):
        if value == self.targetName and not self.subscribeDone:
            self.memory.subscribeToEvent("ALTracker/TargetLost", self.getName(), "onTargetLost")
            self.memory.subscribeToEvent("ALTracker/TargetReached", self.getName(), "onTargetReached")
            self.subscribeDone = True
        elif value != self.targetName and self.subscribeDone:
            self.memory.unsubscribeToEvent("ALTracker/TargetLost", self.getName())
            self.memory.unsubscribeToEvent("ALTracker/TargetReached", self.getName())
            self.subscribeDone = False]]></content></script><Input name="onLoad" type="1" type_size="1" nature="0" inner="1" tooltip="Signal sent when diagram is loaded." id="1" /><Input name="onStart" type="1" type_size="1" nature="2" inner="0" tooltip="Box behavior starts when a signal is received on this input." id="2" /><Input name="onStop" type="1" type_size="1" nature="3" inner="0" tooltip="Box behavior stops when a signal is received on this input." id="3" /><Output name="onStopped" type="1" type_size="1" nature="1" inner="0" tooltip="Signal sent when box behavior is finished." id="4" /><Output name="targetLost" type="1" type_size="1" nature="2" inner="0" tooltip="Signal sent when the target is lost." id="5" /><Output name="targetReached" type="1" type_size="1" nature="2" inner="0" tooltip="Signal sent when the target is reached." id="6" /><Parameter name="Mode" inherits_from_parent="0" content_type="3" value="Head" default_value="Head" custom_choice="0" tooltip="Set tracker mode" id="7"><Choice value="Head" /><Choice value="WholeBody" /><Choice value="Move" /></Parameter><Parameter name="Effector" inherits_from_parent="0" content_type="3" value="None" default_value="None" custom_choice="0" tooltip="Set effector to use for tracking. Head is always used." id="8"><Choice value="None" /><Choice value="Arms" /><Choice value="LArm" /><Choice value="RArm" /></Parameter><Parameter name="Threshold to be sure of the location (%)" inherits_from_parent="0" content_type="1" value="50" default_value="50" min="1" max="100" tooltip="If a sound is localized with a threshold higher than this value, then the sound&#x0A;location will be sent on the output. Else, the robot will consider that the sound is not&#x0A;localized at the right location and he will not take it into account." id="9" /><Parameter name="Distance (m)" inherits_from_parent="0" content_type="2" value="0.3" default_value="0.3" min="0.01" max="5" tooltip="Distance the robot will try to maintain from its target in move modes." id="10" /><Parameter name="Threshold X (m)" inherits_from_parent="0" content_type="2" value="0.1" default_value="0.1" min="0.01" max="1" tooltip="Threshold above which the robot will move to track its target in move modes." id="11" /><Parameter name="Sensitivity" inherits_from_parent="0" content_type="2" value="0.7" default_value="0.7" min="0.01" max="1" tooltip="Sensitivity to adjust the capacity of the robot to locate quiet sounds." id="12" /></Box><Box name="sad1" id="7" localization="8" tooltip="An example of multilanguage dialog implementation" x="294" y="351"><dialogFile>ExampleDialog/ExampleDialog.dlg</dialogFile><bitmap>media/images/box/box-dialog.png</bitmap><script language="4"><content><![CDATA[]]></content></script><Input name="onLoad" type="1" type_size="1" nature="0" inner="1" tooltip="Signal sent when diagram is loaded." id="1" /><Input name="onStart" type="1" type_size="1" nature="2" inner="0" tooltip="Box behavior starts when a signal is received on this input." id="2" /><Input name="onStop" type="1" type_size="1" nature="3" inner="0" tooltip="Box behavior stops when a signal is received on this input." id="3" /><Output name="onStopped" type="1" type_size="1" nature="1" inner="0" tooltip="Signal sent when box behavior is finished." id="4" /><Output name="playAGame" type="1" type_size="1" nature="2" inner="0" tooltip="stimulated using $playAGame=1 in qiChat script when User wants to play a game&#x0A;" id="5" /><Resource name="Speech" type="Lock" timeout="0" /><Resource name="Speech recognition" type="Lock" timeout="0" /></Box><Box name="analyzeEmotion" id="5" localization="8" tooltip="" x="660" y="178"><bitmap>media/images/box/box-python-script.png</bitmap><script language="4"><content><![CDATA[class MyClass(GeneratedClass):
    def __init__(self):
        GeneratedClass.__init__(self)

    def onLoad(self):
        #put initialization code here
        pass

    def onUnload(self):
        #put clean-up code here
        pass

    def onInput_onStart(self, emotions=[]):
        #self.onStopped() #activate the output of the box
        if "sad" in emotions or "angry" in emotions:
            self.somethingWrong()
            pass
        self.onStop()
        pass

    def onInput_onStop(self):
        self.onUnload() #it is recommended to reuse the clean-up as the box is stopped
        self.onStopped() #activate the output of the box]]></content></script><Input name="onLoad" type="1" type_size="1" nature="0" inner="1" tooltip="Signal sent when diagram is loaded." id="1" /><Input name="onStart" type="1" type_size="1" nature="2" inner="0" tooltip="Box behavior starts when a signal is received on this input." id="2" /><Input name="onStop" type="1" type_size="1" nature="3" inner="0" tooltip="Box behavior stops when a signal is received on this input." id="3" /><Output name="onStopped" type="1" type_size="1" nature="1" inner="0" tooltip="Signal sent when box behavior is finished." id="4" /></Box><Link inputowner="1" indexofinput="2" outputowner="0" indexofoutput="2" /><Link inputowner="4" indexofinput="2" outputowner="2" indexofoutput="4" /><Link inputowner="1" indexofinput="2" outputowner="3" indexofoutput="6" /><Link inputowner="1" indexofinput="3" outputowner="1" indexofoutput="6" /><Link inputowner="2" indexofinput="2" outputowner="1" indexofoutput="5" /><Link inputowner="1" indexofinput="2" outputowner="4" indexofoutput="5" /><Link inputowner="5" indexofinput="2" outputowner="4" indexofoutput="4" /></Diagram></BehaviorKeyframe></BehaviorLayer></Timeline></Box></ChoregrapheProject>